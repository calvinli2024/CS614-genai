{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The purpose of this notebook is to fine-tune an open-weight model on a non-trivial dataset and monitor it's performance changes.\n",
    "\n",
    "The model to be finetuned is [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B), due to it's open weight nature and small parameter count.\n",
    "It's low parameter count reduces computational cost and training time, which is a higher priority that usual due to the resource constrained nature of our training environment (Kaggle notebooks).\n",
    "\n",
    "While the `Qwen` series of models are capable of multiple tasks including conversation, reasoning, and text generation, the focus for this particular fine-tuning session will be on improving it's natural language classification abilities by fine-tuning it on the [sh0416/ag_news](https://huggingface.co/datasets/sh0416/ag_news) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Procedure\n",
    "\n",
    "**The primary performance metric is F1.**\n",
    "\n",
    "To optimize resource consumption, the training procedure will leverage a parameter efficient fine-tuning (PEFT) ready version of the model, with LoRA adapters attached. While 6 million parameters is relatively small for a model, it is still too large for two T4 GPU's to train in a reasonable amount of time, hence why PEFT is the go-to technique.\n",
    "\n",
    "The fine-tuning library used was HuggingFace's `Trainer` library.\n",
    "\n",
    "There were five hyperparameters adjusted during training, with two options per parameter, for a total of 32 different hyperparameter combinations.\n",
    "\n",
    "- learning_rate: 1e-4, 1e-3\n",
    "- num_train_epochs: 1, 2\n",
    "- lr_scheduler_type: linear, cosine\n",
    "- gradient_accumulation_steps: 4, 8\n",
    "- weight_decay: 0.01, 0.1\n",
    "\n",
    "While it would have been ideal to do a full-grid search across all combinations, hardware constraints prevented this. Instead, a randomized grid-search was performed where the hyperparameter combinations were shuffled, and the first 16 were then chosen as the ones to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Strategy\n",
    "\n",
    "The code used to evaluate performance can be found in the [Initial Performance](#Initial-Performance) cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The fine-tuned model/adapter can be found in this HuggingFace repository: [cli08/qwen3-0.6-finetuned](https://huggingface.co/cli08/qwen3-0.6-finetuned)\n",
    "\n",
    "The base model had a dismal F1 score of 0.133 on the hold-out set before fine-tuning, but this improved drastically to 0.911 after fine-tuning.\n",
    "\n",
    "|Initial F1|Fine-tuned F1|\n",
    "|-------|----|\n",
    "|0.133|0.911|\n",
    "\n",
    "The best hyperparameter combination was:\n",
    "\n",
    "- learning_rate: 0.001\n",
    "- num_train_epochs: 2\n",
    "- lr_scheduler_type: linear\n",
    "- gradient_accumulation_steps: 4\n",
    "- weight_decay: 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "There was a drastic improvement in the hold-out set's F1 score after fine-tuning, which speaks to the impressive power of PEFT. That being said, the number of parameters was quite small to begin with, and the classification dataset only has four possible labels, so this improvement in F1 might be a result of overfitting to a simple, low-complexity text-classification dataset with little diversity in it's output labels. If there were more output labels, the fine-tuned F1 might not be as high as it currently is. \n",
    "\n",
    "Nevertheless, this experiment was a success, and lends credibility to the strategy that some organizations are using where they fine-tune small language models on proprietary data for internal use cases, instead of renting cloud-hosted LLM's and hoping the providers don't leak data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risks\n",
    "\n",
    "As with all LLM and AI-based technologies, there is a risk of abuse and bias in the model outputs, especially when fed dangerous training data. For example, a malicious actor might use the convenient and accessible nature of open-weight model fine-tuning technology to coerce models into outputting harmful content by fine-tuning it on source materials espousing harmful ideas.\n",
    "\n",
    "It is impossible to prevent malicious actors from fine-tuning an LLM, as datasets with unethical materials are easily compiled, and fine-tuning requires less hardware than full foundation model training. As such, risk mitigation efforts should be focused on teaching end-users to recognize potentially malicious output from an AI model, and not take model output at face-value, regardless of how confident the model might sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility\n",
    "\n",
    "Please note that you will need a HuggingFace Hub account and write-access API token to save the model checkpoints while training. Once you have the API token, save it as a Kaggle notebook secret with the name `HF_TOKEN`. Besides that, the code can be run as is on a free Kaggle notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AI Usage Disclosure\n",
    "The code in this notebook was created with assistance from AI tools. The code has been reviewed and edited by a human. For more information on the extent and nature of AI usage, please contact the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d69d2840"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc729135",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install evaluate peft huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5063f33",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import random\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "219bb5b4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21c40442"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f17fa8ac",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sh0416/ag_news\")\n",
    "\n",
    "dataset = {\n",
    "    \"train\": dataset[\"train\"].shuffle(seed=42).select(range(5000)),\n",
    "    \"valid\": dataset[\"train\"].shuffle(seed=42).select(range(5000, 6000)),\n",
    "    \"test\": dataset[\"test\"].shuffle(seed=42).select(range(2000))\n",
    "}\n",
    "\n",
    "num_labels = len(set(dataset[\"train\"][\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd832b22"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f135bcd2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrhA-fo_kx63"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "085db5df",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Qwen/Qwen3-0.6B\",\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10387705"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe2221af",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "def preprocess(dataset_split, select_columns: List[str]):\n",
    "    def run_tokenizer(row):\n",
    "      return tokenizer(\n",
    "          row[\"text\"],\n",
    "          padding=\"max_length\",\n",
    "          truncation=True,\n",
    "          max_length=max_length,\n",
    "      )\n",
    "\n",
    "    def prepare_columns(row):\n",
    "      row[\"text\"] = row[\"title\"] + \" \" + row[\"description\"]\n",
    "      row[\"label\"] = row[\"label\"] - 1\n",
    "\n",
    "      return row\n",
    "\n",
    "    dataset_split = dataset_split.map(prepare_columns)\n",
    "\n",
    "    encoded_dataset = dataset_split.map(run_tokenizer, batched=True)\n",
    "\n",
    "    encoded_dataset = encoded_dataset.remove_columns(\n",
    "        [col for col in encoded_dataset.column_names if col not in select_columns]\n",
    "    )\n",
    "\n",
    "    return encoded_dataset.with_format(\"torch\")\n",
    "\n",
    "train_dataset = preprocess(dataset['train'], [\"input_ids\", \"attention_mask\", \"label\"])\n",
    "valid_dataset = preprocess(dataset['valid'], [\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset = preprocess(dataset['test'], [\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61ae7f96"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6925906a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    results = accuracy.compute(predictions=preds, references=labels)\n",
    "    results.update(\n",
    "        f1.compute(predictions=preds, references=labels, average=\"macro\")\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8MvJ7BAaqNu"
   },
   "source": [
    "# Initial Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJE-LG3oaqNu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_performance(model: str):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    label_mapping={f\"LABEL_{i}\": i for i in range(num_labels)}\n",
    "\n",
    "    predictions_raw = classifier(list(test_dataset[\"text\"]), batch_size=32)\n",
    "\n",
    "    predicted_labels_str = [pr[\"label\"] for pr in predictions_raw]\n",
    "\n",
    "    predictions = [label_mapping[str_label] for str_label in predicted_labels_str]\n",
    "\n",
    "    final_f1 = evaluate.load(\"f1\").compute(predictions=predictions, references=list(test_dataset[\"label\"]), average=\"macro\")[\"f1\"]\n",
    "\n",
    "    return round(final_f1, 3)\n",
    "\n",
    "initial_f1 = test_performance(\"Qwen/Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8b54633"
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99a6b1ec"
   },
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "527fc936",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"qwen3-0.6-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=0,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "early_stop = EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.02)\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "329362be"
   },
   "source": [
    "## Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08fd5ee0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = []\n",
    "\n",
    "for learning_rate in [1e-4, 1e-3]:\n",
    "    for num_train_epochs in [1, 2]:\n",
    "        for lr_scheduler_type in [\"linear\", \"cosine\"]:\n",
    "            for gradient_accumulation_steps in [4, 8]:\n",
    "                for weight_decay in [0.01, 0.1]:\n",
    "                    hyperparameters.append({\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"num_train_epochs\": num_train_epochs,\n",
    "                        \"lr_scheduler_type\": lr_scheduler_type,\n",
    "                        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "                        \"weight_decay\": weight_decay\n",
    "                    })\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(hyperparameters)\n",
    "hyperparameters = hyperparameters[:len(hyperparameters) // 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "208870ac"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "062e0479",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_single_experiment(\n",
    "    base_training_args: TrainingArguments,\n",
    "    trainer_cls,\n",
    "    trainer_kwargs: Dict[str, Any],\n",
    "    hp_config: Dict[str, float|int],\n",
    "    idx: int\n",
    ") -> Dict[str, Any]:\n",
    "    args_dict = base_training_args.to_dict()\n",
    "    for k, v in hp_config.items():\n",
    "        args_dict[k] = v\n",
    "\n",
    "    training_args = TrainingArguments(**args_dict)\n",
    "\n",
    "    trainer = trainer_cls(\n",
    "        args=training_args,\n",
    "        **trainer_kwargs,\n",
    "    )\n",
    "\n",
    "    train_output = trainer.train()\n",
    "    eval_metrics = trainer.evaluate()\n",
    "\n",
    "    trainer.save_model(f\"checkpoint_{idx}\")\n",
    "\n",
    "    trainer.push_to_hub(\n",
    "        commit_message=f\"checkpoint_{idx}\",\n",
    "        token=UserSecretsClient().get_secret(\"HF_TOKEN\")\n",
    "    )\n",
    "\n",
    "    result = {\n",
    "        \"hp_config\": hp_config,\n",
    "        \"train_samples\": train_output.metrics.get(\"train_samples\", None),\n",
    "        \"eval_metrics\": eval_metrics,\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "def grid_search_hyperparams(\n",
    "    base_training_args: TrainingArguments,\n",
    "    trainer_cls,\n",
    "    trainer_kwargs: Dict[str, Any],\n",
    "    hyperparameters: Dict[int, Dict[str, int|float]],\n",
    "    results_path: str = \"grid_search_results.jsonl\",\n",
    ") -> List[Dict[str, Any]]:\n",
    "    all_results = {}\n",
    "\n",
    "    os.makedirs(os.path.dirname(results_path) or \".\", exist_ok=True)\n",
    "\n",
    "    with open(results_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for idx, combo in enumerate(hyperparameters):\n",
    "            print(\"\\n=== Running config:\", combo, \"===\")\n",
    "\n",
    "            result = run_single_experiment(\n",
    "                base_training_args=base_training_args,\n",
    "                trainer_cls=trainer_cls,\n",
    "                trainer_kwargs=deepcopy(trainer_kwargs),\n",
    "                hp_config=combo,\n",
    "                idx=idx\n",
    "            )\n",
    "\n",
    "            # Persist each result as one JSON line\n",
    "            f_out.write(json.dumps(result) + \"\\n\")\n",
    "            f_out.flush()\n",
    "\n",
    "            all_results[idx] = result\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd6829fa"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa47f6ab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = grid_search_hyperparams(\n",
    "    base_training_args=training_arguments,\n",
    "    trainer_cls=Trainer,\n",
    "    trainer_kwargs=trainer_kwargs,\n",
    "    hyperparameters=hyperparameters,\n",
    "    results_path=\"grid_search_results.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fpkVLBxwETe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_config = max(results.items(), key=lambda r: r[1][\"eval_metrics\"].get(\"eval_f1\", 0.0))\n",
    "\n",
    "print(\"Best index:\", best_config[0])\n",
    "print(\"Best config:\", best_config[1][\"hp_config\"])\n",
    "print(\"Best metrics:\", best_config[1][\"eval_metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "466e425e"
   },
   "source": [
    "# Post Fine-Tuning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCWe6sUbaqNx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "finetuned_f1 = test_performance(f\"./checkpoint_{best_config[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5QXBaoTaqNx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"\"\"\n",
    "|Initial|Post|\n",
    "|-------|----|\n",
    "|{initial_f1}|{finetuned_f1}|\n",
    "\"\"\".format(initial_f1=initial_f1, finetuned_f1=finetuned_f1)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
