{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nThe purpose of this notebook is to fine-tune an open-weight model on a non-trivial dataset and monitor it's performance changes.\n\nThe model to be finetuned is [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B), due to it's open weight nature and small parameter count.\nIt's low parameter count reduces computational cost and training time, which is a higher priority that usual due to the resource constrained nature of our training environment (Kaggle notebooks).\n\nWhile the `Qwen` series of models are capable of multiple tasks including conversation, reasoning, and text generation, the focus for this particular fine-tuning session will be on improving it's natural language classification abilities by fine-tuning it on the [sh0416/ag_news](https://huggingface.co/datasets/sh0416/ag_news) dataset. \n\n**The primary performance metric is F1.**\n\nTo further optimize resource consumption, the training procedure will leverage a parameter efficient fine-tuning (PEFT) ready version of the model, with LoRA adapters attached. While 6 million parameters is relatively small for a model, it is still too large for two T4 GPU's to train in a reasonable amount of time, hence why PEFT is the go-to technique.\n\nThe fine-tuning library used was HuggingFace's `Trainer` library.\n\nThere were five hyperparameters adjusted during training, with two options per parameter, for a total of 32 different hyperparameter combinations.\n\n- learning_rate: 1e-4, 1e-3\n- num_train_epochs: 1, 2\n- lr_scheduler_type: linear, cosine\n- gradient_accumulation_steps: 4, 8\n- weight_decay: 0.01, 0.1\n\nWhile it would have been ideal to do a full-grid search across all combinations, hardware constraints prevented this. Instead, a randomized grid-search was performed where the hyperparameter combinations were shuffled, and the first 16 were then chosen as the ones to test.","metadata":{}},{"cell_type":"markdown","source":"# Evaluation Strategy\n\nThe code used to evaluate performance can be found in [this cell.](#Initial-Performance)","metadata":{}},{"cell_type":"markdown","source":"# Results\n\nThe fine-tuned model/adapter can be found in this HuggingFace repository: [cli08/qwen3-0.6-finetuned](https://huggingface.co/cli08/qwen3-0.6-finetuned)\n\nThe base model had a dismal F1 score of 0.253 on the hold-out set before fine-tuning, but this improved drastically to 0.908 after fine-tuning.\n\n|Initial F1|Fine-tuned F1|\n|-------|----|\n|0.253|0.908|\n\nThe best hyperparameter combination was:\n\n- learning_rate: 0.001\n- num_train_epochs: 2\n- lr_scheduler_type: linear\n- gradient_accumulation_steps: 4\n- weight_decay: 0.01","metadata":{}},{"cell_type":"markdown","source":"# Analysis\n\nThere was a drastic improvement in the hold-out set's F1 score after fine-tuning, which speaks to the impressive power of PEFT. That being said, the number of parameters was quite small to begin with, and the classification dataset only has four possible labels, so this improvement in F1 might be a result of overfitting to a simple, low-complexity text-classification dataset with little diversity in it's output labels. If there were more output labels, the fine-tuned F1 might not be as high as it currently is. \n\nNevertheless, this experiment was a success, and lends credibility to the strategy that some organizations are using where they fine-tune small language models on proprietary data for internal use cases, instead of renting cloud-hosted LLM's and hoping the providers don't leak data.","metadata":{}},{"cell_type":"markdown","source":"# Risks\n\nAs with all LLM and AI-based technologies, there is a risk of abuse and bias in the model outputs, especially when fed dangerous training data. For example, a malicious actor might use the convenient and accessible nature of open-weight model fine-tuning technology to coerce models into outputting harmful content by fine-tuning it on source materials espousing harmful ideas.","metadata":{}},{"cell_type":"markdown","source":"##### AI Usage Disclosure\nThe code in this notebook was created with assistance from AI tools. The code has been reviewed and edited by a human. For more information on the extent and nature of AI usage, please contact the author.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Packages","metadata":{"id":"d69d2840"}},{"cell_type":"code","source":"%pip install evaluate peft huggingface-hub","metadata":{"id":"fc729135","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Any\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    AutoModelForCausalLM,\n    pipeline,\n    EarlyStoppingCallback\n)\n\nimport evaluate\nfrom evaluate import evaluator\n\nimport itertools\nimport json\nfrom copy import deepcopy\n\nfrom peft import LoraConfig, TaskType, get_peft_model\n\nfrom IPython.display import display, Markdown\n\nimport random\n\nfrom huggingface_hub import HfApi\n\nfrom kaggle_secrets import UserSecretsClient","metadata":{"id":"a5063f33","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"id":"219bb5b4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"21c40442"}},{"cell_type":"code","source":"dataset = load_dataset(\"sh0416/ag_news\")\n\ndataset = {\n    \"train\": dataset[\"train\"].shuffle(seed=42).select(range(5000)),\n    \"valid\": dataset[\"train\"].shuffle(seed=42).select(range(5000, 6000)),\n    \"test\": dataset[\"test\"].shuffle(seed=42).select(range(2000))\n}\n\nnum_labels = len(set(dataset[\"train\"][\"label\"]))","metadata":{"id":"f17fa8ac","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenizer","metadata":{"id":"dd832b22"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"f135bcd2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{"id":"xrhA-fo_kx63"}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"Qwen/Qwen3-0.6B\",\n    num_labels=num_labels,\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1\n)\n\nmodel = get_peft_model(model, lora_config)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)","metadata":{"id":"085db5df","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess","metadata":{"id":"10387705"}},{"cell_type":"code","source":"max_length = 256\n\ndef preprocess(dataset_split, select_columns: List[str]):\n    def run_tokenizer(row):\n      return tokenizer(\n          row[\"text\"],\n          padding=\"max_length\",\n          truncation=True,\n          max_length=max_length,\n      )\n\n    def prepare_columns(row):\n      row[\"text\"] = row[\"title\"] + \" \" + row[\"description\"]\n      row[\"label\"] = row[\"label\"] - 1\n\n      return row\n\n    dataset_split = dataset_split.map(prepare_columns)\n\n    encoded_dataset = dataset_split.map(run_tokenizer, batched=True)\n\n    encoded_dataset = encoded_dataset.remove_columns(\n        [col for col in encoded_dataset.column_names if col not in select_columns]\n    )\n\n    return encoded_dataset.with_format(\"torch\")\n\ntrain_dataset = preprocess(dataset['train'], [\"input_ids\", \"attention_mask\", \"label\"])\nvalid_dataset = preprocess(dataset['valid'], [\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset = preprocess(dataset['test'], [\"label\", \"text\"])","metadata":{"id":"fe2221af","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics","metadata":{"id":"61ae7f96"}},{"cell_type":"code","source":"accuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    results = accuracy.compute(predictions=preds, references=labels)\n    results.update(\n        f1.compute(predictions=preds, references=labels, average=\"macro\")\n    )\n\n    return results","metadata":{"id":"6925906a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initial Performance","metadata":{"id":"g8MvJ7BAaqNu"}},{"cell_type":"code","source":"def test_performance(model: str):\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model,\n        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n        pad_token_id=tokenizer.pad_token_id,\n        num_labels=num_labels\n    )\n\n    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n\n    label_mapping={f\"LABEL_{i}\": i for i in range(num_labels)}\n\n    predictions_raw = classifier(list(test_dataset[\"text\"]), batch_size=32)\n\n    predicted_labels_str = [pr[\"label\"] for pr in predictions_raw]\n\n    predictions = [label_mapping[str_label] for str_label in predicted_labels_str]\n\n    final_f1 = evaluate.load(\"f1\").compute(predictions=predictions, references=list(test_dataset[\"label\"]), average=\"macro\")[\"f1\"]\n\n    return round(final_f1, 3)\n\ninitial_f1 = test_performance(\"Qwen/Qwen3-0.6B\")","metadata":{"trusted":true,"id":"LJE-LG3oaqNu"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{"id":"d8b54633"}},{"cell_type":"markdown","source":"## Training Arguments","metadata":{"id":"99a6b1ec"}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"qwen3-0.6-finetuned\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=0,\n    lr_scheduler_type=\"cosine\",\n    gradient_accumulation_steps=4,\n    fp16=False,\n    report_to=\"none\",\n    save_steps=50,\n    eval_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True\n)\n\nearly_stop = EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.02)\n\ntrainer_kwargs = dict(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[early_stop]\n)","metadata":{"id":"527fc936","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Search Space","metadata":{"id":"329362be"}},{"cell_type":"code","source":"hyperparameters = []\n\nfor learning_rate in [1e-4, 1e-3]:\n    for num_train_epochs in [1, 2]:\n        for lr_scheduler_type in [\"linear\", \"cosine\"]:\n            for gradient_accumulation_steps in [4, 8]:\n                for weight_decay in [0.01, 0.1]:\n                    hyperparameters.append({\n                        \"learning_rate\": learning_rate,\n                        \"num_train_epochs\": num_train_epochs,\n                        \"lr_scheduler_type\": lr_scheduler_type,\n                        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n                        \"weight_decay\": weight_decay\n                    })\n\nrandom.seed(42)\nrandom.shuffle(hyperparameters)\nhyperparameters = hyperparameters[:len(hyperparameters) // 2]","metadata":{"id":"08fd5ee0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper Functions","metadata":{"id":"208870ac"}},{"cell_type":"code","source":"def run_single_experiment(\n    base_training_args: TrainingArguments,\n    trainer_cls,\n    trainer_kwargs: Dict[str, Any],\n    hp_config: Dict[str, float|int],\n    idx: int\n) -> Dict[str, Any]:\n    args_dict = base_training_args.to_dict()\n    for k, v in hp_config.items():\n        args_dict[k] = v\n\n    training_args = TrainingArguments(**args_dict)\n\n    trainer = trainer_cls(\n        args=training_args,\n        **trainer_kwargs,\n    )\n\n    train_output = trainer.train()\n    eval_metrics = trainer.evaluate()\n\n    trainer.save_model(f\"checkpoint_{idx}\")\n\n    trainer.push_to_hub(\n        commit_message=f\"checkpoint_{idx}\",\n        token=UserSecretsClient().get_secret(\"HF_TOKEN\")\n    )\n\n    result = {\n        \"hp_config\": hp_config,\n        \"train_samples\": train_output.metrics.get(\"train_samples\", None),\n        \"eval_metrics\": eval_metrics,\n    }\n\n    return result\n\ndef grid_search_hyperparams(\n    base_training_args: TrainingArguments,\n    trainer_cls,\n    trainer_kwargs: Dict[str, Any],\n    hyperparameters: Dict[int, Dict[str, int|float]],\n    results_path: str = \"grid_search_results.jsonl\",\n) -> List[Dict[str, Any]]:\n    all_results = {}\n\n    os.makedirs(os.path.dirname(results_path) or \".\", exist_ok=True)\n\n    with open(results_path, \"w\", encoding=\"utf-8\") as f_out:\n        for idx, combo in enumerate(hyperparameters):\n            print(\"\\n=== Running config:\", combo, \"===\")\n\n            result = run_single_experiment(\n                base_training_args=base_training_args,\n                trainer_cls=trainer_cls,\n                trainer_kwargs=deepcopy(trainer_kwargs),\n                hp_config=combo,\n                idx=idx\n            )\n\n            # Persist each result as one JSON line\n            f_out.write(json.dumps(result) + \"\\n\")\n            f_out.flush()\n\n            all_results[idx] = result\n\n    return all_results","metadata":{"id":"062e0479","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{"id":"bd6829fa"}},{"cell_type":"code","source":"results = grid_search_hyperparams(\n    base_training_args=training_arguments,\n    trainer_cls=Trainer,\n    trainer_kwargs=trainer_kwargs,\n    hyperparameters=hyperparameters,\n    results_path=\"grid_search_results.jsonl\",\n)","metadata":{"id":"fa47f6ab","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_config = max(results.items(), key=lambda r: r[1][\"eval_metrics\"].get(\"eval_f1\", 0.0))\n\nprint(\"Best index:\", best_config[0])\nprint(\"Best config:\", best_config[1][\"hp_config\"])\nprint(\"Best metrics:\", best_config[1][\"eval_metrics\"])","metadata":{"trusted":true,"id":"6fpkVLBxwETe"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Post Fine-Tuning Performance","metadata":{"id":"466e425e"}},{"cell_type":"code","source":"finetuned_f1 = test_performance(f\"./checkpoint_{best_config[0]}\")","metadata":{"trusted":true,"id":"qCWe6sUbaqNx"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(Markdown(\"\"\"\n|Initial|Post|\n|-------|----|\n|{initial_f1}|{finetuned_f1}|\n\"\"\".format(initial_f1=initial_f1, finetuned_f1=finetuned_f1)))","metadata":{"trusted":true,"id":"o5QXBaoTaqNx"},"outputs":[],"execution_count":null}]}