{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/calvinli2024/CS614-genai/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"# Packages","metadata":{"id":"d69d2840"}},{"cell_type":"code","source":"%pip install evaluate peft huggingface-hub","metadata":{"id":"fc729135","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T10:48:59.149074Z","iopub.execute_input":"2026-02-21T10:48:59.149373Z","iopub.status.idle":"2026-02-21T10:49:03.218481Z","shell.execute_reply.started":"2026-02-21T10:48:59.149337Z","shell.execute_reply":"2026-02-21T10:49:03.217634Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (26.0rc2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (3.20.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.2.1rc0)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Any\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    AutoModelForCausalLM,\n    pipeline,\n    EarlyStoppingCallback\n)\n\nimport evaluate\nfrom evaluate import evaluator\n\nimport itertools\nimport json\nfrom copy import deepcopy\n\nfrom peft import LoraConfig, TaskType, get_peft_model\n\nfrom IPython.display import display, Markdown\n\nimport random\n\nfrom huggingface_hub import HfApi\n\nfrom kaggle_secrets import UserSecretsClient","metadata":{"id":"a5063f33","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T10:49:03.220091Z","iopub.execute_input":"2026-02-21T10:49:03.220365Z","iopub.status.idle":"2026-02-21T10:49:14.323778Z","shell.execute_reply.started":"2026-02-21T10:49:03.220334Z","shell.execute_reply":"2026-02-21T10:49:14.323058Z"}},"outputs":[{"name":"stderr","text":"2026-02-21 10:49:10.189077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771670950.212913     906 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771670950.220513     906 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771670950.240412     906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771670950.240434     906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771670950.240437     906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771670950.240439     906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"id":"219bb5b4","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T10:49:14.326044Z","iopub.execute_input":"2026-02-21T10:49:14.326833Z","iopub.status.idle":"2026-02-21T10:49:14.330590Z","shell.execute_reply.started":"2026-02-21T10:49:14.326802Z","shell.execute_reply":"2026-02-21T10:49:14.329841Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"21c40442"}},{"cell_type":"code","source":"dataset = load_dataset(\"sh0416/ag_news\")\n\ndataset = {\n    \"train\": dataset[\"train\"].shuffle(seed=42).select(range(5000)),\n    \"valid\": dataset[\"train\"].shuffle(seed=42).select(range(5000, 6000)),\n    \"test\": dataset[\"test\"].shuffle(seed=42).select(range(2000))\n}\n\nnum_labels = len(set(dataset[\"train\"][\"label\"]))","metadata":{"id":"f17fa8ac","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T10:49:14.331547Z","iopub.execute_input":"2026-02-21T10:49:14.331898Z","iopub.status.idle":"2026-02-21T10:49:15.327910Z","shell.execute_reply.started":"2026-02-21T10:49:14.331861Z","shell.execute_reply":"2026-02-21T10:49:15.326965Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Tokenizer","metadata":{"id":"dd832b22"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"f135bcd2","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T10:49:15.329116Z","iopub.execute_input":"2026-02-21T10:49:15.330031Z","iopub.status.idle":"2026-02-21T10:49:15.927130Z","shell.execute_reply.started":"2026-02-21T10:49:15.329998Z","shell.execute_reply":"2026-02-21T10:49:15.926488Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Model","metadata":{"id":"xrhA-fo_kx63"}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"Qwen/Qwen3-0.6B\",\n    num_labels=num_labels,\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False, # set to False for training\n    r=8, # dimension of the smaller matrices\n    lora_alpha=32, # scaling factor\n    lora_dropout=0.1 # dropout of LoRA layers\n)\n\nmodel = get_peft_model(model, lora_config)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)","metadata":{"id":"085db5df","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T10:49:15.928112Z","iopub.execute_input":"2026-02-21T10:49:15.928382Z","iopub.status.idle":"2026-02-21T10:49:16.878633Z","shell.execute_reply.started":"2026-02-21T10:49:15.928355Z","shell.execute_reply":"2026-02-21T10:49:16.877882Z"}},"outputs":[{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\nSome weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): Qwen3ForSequenceClassification(\n      (model): Qwen3Model(\n        (embed_tokens): Embedding(151936, 1024, padding_idx=151643)\n        (layers): ModuleList(\n          (0-27): 28 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n        )\n        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (rotary_emb): Qwen3RotaryEmbedding()\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=1024, out_features=4, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=1024, out_features=4, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Preprocess","metadata":{"id":"10387705"}},{"cell_type":"code","source":"max_length = 256\n\ndef preprocess(dataset_split, select_columns: List[str]):\n    def run_tokenizer(row):\n      return tokenizer(\n          row[\"text\"],\n          padding=\"max_length\",\n          truncation=True,\n          max_length=max_length,\n      )\n\n    def prepare_columns(row):\n      row[\"text\"] = row[\"title\"] + \" \" + row[\"description\"]\n      row[\"label\"] = row[\"label\"] - 1\n\n      return row\n\n    dataset_split = dataset_split.map(prepare_columns)\n\n    encoded_dataset = dataset_split.map(run_tokenizer, batched=True)\n\n    encoded_dataset = encoded_dataset.remove_columns(\n        [col for col in encoded_dataset.column_names if col not in select_columns]\n    )\n\n    return encoded_dataset.with_format(\"torch\")\n\ntrain_dataset = preprocess(dataset['train'], [\"input_ids\", \"attention_mask\", \"label\"])\nvalid_dataset = preprocess(dataset['valid'], [\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset = preprocess(dataset['test'], [\"label\", \"text\"])","metadata":{"id":"fe2221af","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T10:49:16.879829Z","iopub.execute_input":"2026-02-21T10:49:16.880196Z","iopub.status.idle":"2026-02-21T10:49:17.243131Z","shell.execute_reply.started":"2026-02-21T10:49:16.880152Z","shell.execute_reply":"2026-02-21T10:49:17.242190Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Metrics","metadata":{"id":"61ae7f96"}},{"cell_type":"code","source":"accuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    results = accuracy.compute(predictions=preds, references=labels)\n    results.update(\n        f1.compute(predictions=preds, references=labels, average=\"macro\")\n    )\n\n    return results","metadata":{"id":"6925906a","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T10:49:17.244224Z","iopub.execute_input":"2026-02-21T10:49:17.244496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initial Performance","metadata":{"id":"g8MvJ7BAaqNu"}},{"cell_type":"code","source":"def test_performance(model: str):\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model,\n        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n        pad_token_id=tokenizer.pad_token_id,\n        num_labels=num_labels\n    )\n\n    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n\n    label_mapping={f\"LABEL_{i}\": i for i in range(num_labels)}\n\n    predictions_raw = classifier(list(test_dataset[\"text\"]), batch_size=32)\n\n    predicted_labels_str = [pr[\"label\"] for pr in predictions_raw]\n\n    predictions = [label_mapping[str_label] for str_label in predicted_labels_str]\n\n    final_f1 = evaluate.load(\"f1\").compute(predictions=predictions, references=list(test_dataset[\"label\"]), average=\"macro\")[\"f1\"]\n\n    return final_f1\n\ninitial_f1 = test_performance(\"Qwen/Qwen3-0.6B\")","metadata":{"trusted":true,"id":"LJE-LG3oaqNu"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{"id":"d8b54633"}},{"cell_type":"markdown","source":"## Training Arguments","metadata":{"id":"99a6b1ec"}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"qwen3-0.6-finetuned\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=0,\n    lr_scheduler_type=\"cosine\",\n    gradient_accumulation_steps=4,\n    fp16=False,\n    report_to=\"none\",\n    save_steps=50,\n    eval_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True\n)\n\nearly_stop = EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.02)\n\ntrainer_kwargs = dict(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[early_stop]\n)","metadata":{"id":"527fc936","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Search Space","metadata":{"id":"329362be"}},{"cell_type":"code","source":"hyperparameters = []\n\nfor learning_rate in [1e-4, 1e-3]:\n    for num_train_epochs in [1, 2]:\n        for lr_scheduler_type in [\"linear\", \"cosine\"]:\n            for gradient_accumulation_steps in [4, 8]:\n                for weight_decay in [0.01, 0.1]:\n                    hyperparameters.append({\n                        \"learning_rate\": learning_rate,\n                        \"num_train_epochs\": num_train_epochs,\n                        \"lr_scheduler_type\": lr_scheduler_type,\n                        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n                        \"weight_decay\": weight_decay\n                    })\n\nrandom.seed(42)\nrandom.shuffle(hyperparameters)\nhyperparameters = hyperparameters[:len(hyperparameters) // 2]","metadata":{"id":"08fd5ee0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper Functions","metadata":{"id":"208870ac"}},{"cell_type":"code","source":"def run_single_experiment(\n    base_training_args: TrainingArguments,\n    trainer_cls,\n    trainer_kwargs: Dict[str, Any],\n    hp_config: Dict[str, float|int],\n    idx: int\n) -> Dict[str, Any]:\n    args_dict = base_training_args.to_dict()\n    for k, v in hp_config.items():\n        args_dict[k] = v\n\n    training_args = TrainingArguments(**args_dict)\n\n    trainer = trainer_cls(\n        args=training_args,\n        **trainer_kwargs,\n    )\n\n    train_output = trainer.train()\n    eval_metrics = trainer.evaluate()\n\n    trainer.save_model(f\"checkpoint_{idx}\")\n\n    trainer.push_to_hub(\n        repo_id=\"cli08/qwen3-0/6-finetuned\",\n        token=UserSecretsClient().get_secret(\"HF_TOKEN\")\n    )\n\n    result = {\n        \"hp_config\": hp_config,\n        \"train_samples\": train_output.metrics.get(\"train_samples\", None),\n        \"eval_metrics\": eval_metrics,\n    }\n\n    return result\n\ndef grid_search_hyperparams(\n    base_training_args: TrainingArguments,\n    trainer_cls,\n    trainer_kwargs: Dict[str, Any],\n    hyperparameters: Dict[int, Dict[str, int|float]],\n    results_path: str = \"grid_search_results.jsonl\",\n) -> List[Dict[str, Any]]:\n    all_results = {}\n\n    os.makedirs(os.path.dirname(results_path) or \".\", exist_ok=True)\n\n    with open(results_path, \"w\", encoding=\"utf-8\") as f_out:\n        for idx, combo in enumerate(hyperparameters):\n            print(\"\\n=== Running config:\", combo, \"===\")\n\n            result = run_single_experiment(\n                base_training_args=base_training_args,\n                trainer_cls=trainer_cls,\n                trainer_kwargs=deepcopy(trainer_kwargs),\n                hp_config=combo,\n                idx=idx\n            )\n\n            # Persist each result as one JSON line\n            f_out.write(json.dumps(result) + \"\\n\")\n            f_out.flush()\n\n            all_results[idx] = result\n\n    return all_results","metadata":{"id":"062e0479","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{"id":"bd6829fa"}},{"cell_type":"code","source":"results = grid_search_hyperparams(\n    base_training_args=training_arguments,\n    trainer_cls=Trainer,\n    trainer_kwargs=trainer_kwargs,\n    hyperparameters=hyperparameters,\n    results_path=\"grid_search_results.jsonl\",\n)","metadata":{"id":"fa47f6ab","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_config = max(results.items(), key=lambda r: r[1][\"eval_metrics\"].get(\"eval_f1\", 0.0))\n\nprint(\"Best index:\", best_config[0])\nprint(\"Best config:\", best_config[1][\"hp_config\"])\nprint(\"Best metrics:\", best_config[1][\"eval_metrics\"])","metadata":{"trusted":true,"id":"6fpkVLBxwETe"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Post Fine-Tuning Performance","metadata":{"id":"466e425e"}},{"cell_type":"code","source":"finetuned_f1 = test_performance(f\"./checkpoint_{best_config[0]}\")","metadata":{"trusted":true,"id":"qCWe6sUbaqNx"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(Markdown(\"\"\"\n|Initial|Post|\n|-------|----|\n|{initial_f1}|{finetuned_f1}|\n\"\"\").format(initial_f1=initial_f1, finetuned_f1=finetuned_f1))","metadata":{"trusted":true,"id":"o5QXBaoTaqNx"},"outputs":[],"execution_count":null}]}