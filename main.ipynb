{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calvinli2024/CS614-genai/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "d69d2840"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install evaluate peft"
      ],
      "metadata": {
        "id": "fc729135",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "import evaluate\n",
        "from evaluate import evaluator\n",
        "\n",
        "import itertools\n",
        "import json\n",
        "from copy import deepcopy\n",
        "\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "a5063f33",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "219bb5b4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "21c40442"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"sh0416/ag_news\")\n",
        "\n",
        "dataset = {\n",
        "    \"train\": dataset[\"train\"].shuffle(seed=42).select(range(5000)),\n",
        "    \"valid\": dataset[\"train\"].shuffle(seed=42).select(range(5000, 6000)),\n",
        "    \"test\": dataset[\"test\"].shuffle(seed=42).select(range(2000))\n",
        "}\n",
        "\n",
        "num_labels = len(set(dataset[\"train\"][\"label\"]))"
      ],
      "metadata": {
        "id": "f17fa8ac",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "dd832b22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
        "\n",
        "# Ensure we have a padding token (Qwen is a causal LM)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "f135bcd2",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "xrhA-fo_kx63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"Qwen/Qwen3-0.6B\",\n",
        "    num_labels=num_labels,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False, # set to False for training\n",
        "    r=8, # dimension of the smaller matrices\n",
        "    lora_alpha=32, # scaling factor\n",
        "    lora_dropout=0.1 # dropout of LoRA layers\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "085db5df",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "10387705"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 256\n",
        "\n",
        "def preprocess(dataset_split, select_columns: List[str]):\n",
        "    def run_tokenizer(row):\n",
        "      return tokenizer(\n",
        "          row[\"text\"],\n",
        "          padding=\"max_length\",\n",
        "          truncation=True,\n",
        "          max_length=max_length,\n",
        "      )\n",
        "\n",
        "    def prepare_columns(row):\n",
        "      row[\"text\"] = row[\"title\"] + \" \" + row[\"description\"]\n",
        "      row[\"label\"] = row[\"label\"] - 1\n",
        "\n",
        "      return row\n",
        "\n",
        "    dataset_split = dataset_split.map(prepare_columns)\n",
        "\n",
        "    encoded_dataset = dataset_split.map(run_tokenizer, batched=True)\n",
        "\n",
        "    encoded_dataset = encoded_dataset.remove_columns(\n",
        "        [col for col in encoded_dataset.column_names if col not in select_columns]\n",
        "    )\n",
        "\n",
        "    return encoded_dataset.with_format(\"torch\")\n",
        "\n",
        "train_dataset = preprocess(dataset['train'], [\"input_ids\", \"attention_mask\", \"label\"])\n",
        "valid_dataset = preprocess(dataset['valid'], [\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_dataset = preprocess(dataset['test'], [\"label\", \"text\"])"
      ],
      "metadata": {
        "id": "fe2221af",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "61ae7f96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    results = accuracy.compute(predictions=preds, references=labels)\n",
        "    results.update(\n",
        "        f1.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "6925906a",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Performance"
      ],
      "metadata": {
        "id": "g8MvJ7BAaqNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_performance(model: str):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model,\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    label_mapping={f\"LABEL_{i}\": i for i in range(num_labels)}\n",
        "\n",
        "    predictions_raw = classifier(list(test_dataset[\"text\"]), batch_size=32)\n",
        "\n",
        "    predicted_labels_str = [pr[\"label\"] for pr in predictions_raw]\n",
        "\n",
        "    predictions = [label_mapping[str_label] for str_label in predicted_labels_str]\n",
        "\n",
        "    final_f1 = evaluate.load(\"f1\").compute(predictions=predictions, references=list(test_dataset[\"label\"]), average=\"macro\")[\"f1\"]\n",
        "\n",
        "    return final_f1\n",
        "\n",
        "initial_f1 = test_performance(\"Qwen/Qwen3-0.6B\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LJE-LG3oaqNu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "d8b54633"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Arguments"
      ],
      "metadata": {
        "id": "99a6b1ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./qwen3_agnews_grid\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=0,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=False,\n",
        "    report_to=\"none\",\n",
        "    save_steps=50,\n",
        "    eval_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "early_stop = EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.02)\n",
        "\n",
        "trainer_kwargs = dict(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ],
      "metadata": {
        "id": "527fc936",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search Space"
      ],
      "metadata": {
        "id": "329362be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = []\n",
        "\n",
        "idx = 0\n",
        "for learning_rate in [1e-4, 1e-3]:\n",
        "    for num_train_epochs in [1, 2]:\n",
        "        for lr_scheduler_type in [\"linear\", \"cosine\"]:\n",
        "            for gradient_accumulation_steps in [4, 8]:\n",
        "                for weight_decay in [0.01, 0.1]:\n",
        "                    hyperparameters[idx] = {\n",
        "                        \"learning_rate\": learning_rate,\n",
        "                        \"num_train_epochs\": num_train_epochs,\n",
        "                        \"lr_scheduler_type\": lr_scheduler_type,\n",
        "                        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "                        \"weight_decay\": weight_decay\n",
        "                    }\n",
        "\n",
        "                    idx += 1\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(hyperparameters)\n",
        "hyperparameters = hyperparameters[:len(hyperparameters) // 2]"
      ],
      "metadata": {
        "id": "08fd5ee0",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "208870ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_single_experiment(\n",
        "    base_training_args: TrainingArguments,\n",
        "    trainer_cls,\n",
        "    trainer_kwargs: Dict[str, Any],\n",
        "    hp_config: Dict[str, float|int],\n",
        "    idx: int\n",
        ") -> Dict[str, Any]:\n",
        "    args_dict = base_training_args.to_dict()\n",
        "    for k, v in hp_config.items():\n",
        "        args_dict[k] = v\n",
        "\n",
        "    training_args = TrainingArguments(**args_dict)\n",
        "\n",
        "    trainer = trainer_cls(\n",
        "        args=training_args,\n",
        "        **trainer_kwargs,\n",
        "    )\n",
        "\n",
        "    train_output = trainer.train()\n",
        "    eval_metrics = trainer.evaluate()\n",
        "\n",
        "    trainer.save_model(f\"checkpoint_{idx}\")\n",
        "\n",
        "    result = {\n",
        "        \"hp_config\": hp_config,\n",
        "        \"train_samples\": train_output.metrics.get(\"train_samples\", None),\n",
        "        \"eval_metrics\": eval_metrics,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "def grid_search_hyperparams(\n",
        "    base_training_args: TrainingArguments,\n",
        "    trainer_cls,\n",
        "    trainer_kwargs: Dict[str, Any],\n",
        "    hyperparameters: Dict[int, Dict[str, int|float]],\n",
        "    results_path: str = \"grid_search_results.jsonl\",\n",
        ") -> List[Dict[str, Any]]:\n",
        "    all_results = {}\n",
        "\n",
        "    os.makedirs(os.path.dirname(results_path) or \".\", exist_ok=True)\n",
        "\n",
        "    with open(results_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "        for idx, combo in enumerate(hyperparameters):\n",
        "            print(\"\\n=== Running config:\", combo, \"===\")\n",
        "\n",
        "            result = run_single_experiment(\n",
        "                base_training_args=base_training_args,\n",
        "                trainer_cls=trainer_cls,\n",
        "                trainer_kwargs=deepcopy(trainer_kwargs),\n",
        "                hp_config=combo,\n",
        "                idx=idx\n",
        "            )\n",
        "\n",
        "            # Persist each result as one JSON line\n",
        "            f_out.write(json.dumps(result) + \"\\n\")\n",
        "            f_out.flush()\n",
        "\n",
        "            all_results[idx] = result\n",
        "\n",
        "    return all_results"
      ],
      "metadata": {
        "id": "062e0479",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "bd6829fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = grid_search_hyperparams(\n",
        "    base_training_args=training_arguments,\n",
        "    trainer_cls=Trainer,\n",
        "    trainer_kwargs=trainer_kwargs,\n",
        "    hyperparameters=hyperparameters,\n",
        "    results_path=\"grid_search_results.jsonl\",\n",
        ")"
      ],
      "metadata": {
        "id": "fa47f6ab",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "best_config = max(results.items(), key=lambda r: r[1][\"eval_metrics\"].get(\"eval_f1\", 0.0))\n",
        "\n",
        "print(\"Best index:\", best_config[0])\n",
        "print(\"Best config:\", best_config[1][\"hp_config\"])\n",
        "print(\"Best metrics:\", best_config[1][\"eval_metrics\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "6fpkVLBxwETe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Fine-Tuning Performance"
      ],
      "metadata": {
        "id": "466e425e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_f1 = test_performance(f\"./checkpoint_{best_config[0]}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "qCWe6sUbaqNx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(\"\"\"\n",
        "|Initial|Post|\n",
        "|-------|----|\n",
        "|{initial_f1}|{finetuned_f1}|\n",
        "\"\"\").format(initial_f1=initial_f1, finetuned_f1=finetuned_f1))"
      ],
      "metadata": {
        "trusted": true,
        "id": "o5QXBaoTaqNx"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}