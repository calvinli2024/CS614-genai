{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/calvinli2024/CS614-genai/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"# Packages","metadata":{"id":"d69d2840"}},{"cell_type":"code","source":"%pip install evaluate peft","metadata":{"id":"fc729135","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:01:42.665870Z","iopub.execute_input":"2026-02-17T09:01:42.666199Z","iopub.status.idle":"2026-02-17T09:01:47.294570Z","shell.execute_reply.started":"2026-02-17T09:01:42.666171Z","shell.execute_reply":"2026-02-17T09:01:47.293790Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (26.0rc2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.3)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.6\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Any\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    AutoModelForCausalLM,\n    pipeline\n)\n\nimport evaluate\nfrom evaluate import evaluator\n\nimport itertools\nimport json\nfrom copy import deepcopy\n\nfrom peft import LoraConfig, TaskType, get_peft_model","metadata":{"id":"a5063f33","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:01:47.296356Z","iopub.execute_input":"2026-02-17T09:01:47.296693Z","iopub.status.idle":"2026-02-17T09:02:19.084134Z","shell.execute_reply.started":"2026-02-17T09:01:47.296663Z","shell.execute_reply":"2026-02-17T09:02:19.083595Z"}},"outputs":[{"name":"stderr","text":"2026-02-17 09:02:03.252222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771318923.486066      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771318923.550289      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771318924.090033      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771318924.090073      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771318924.090076      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771318924.090079      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"id":"219bb5b4","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:19.084878Z","iopub.execute_input":"2026-02-17T09:02:19.085451Z","iopub.status.idle":"2026-02-17T09:02:19.088906Z","shell.execute_reply.started":"2026-02-17T09:02:19.085422Z","shell.execute_reply":"2026-02-17T09:02:19.088306Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"21c40442"}},{"cell_type":"code","source":"dataset = load_dataset(\"sh0416/ag_news\")\n\ndataset = {\n    \"train\": dataset[\"train\"].shuffle(seed=42).select(range(5000)),\n    \"valid\": dataset[\"train\"].shuffle(seed=42).select(range(5000, 6000)),\n    \"test\": dataset[\"test\"].shuffle(seed=42).select(range(2000))\n}\n\nnum_labels = len(set(dataset[\"train\"][\"label\"]))","metadata":{"id":"f17fa8ac","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:19.089856Z","iopub.execute_input":"2026-02-17T09:02:19.090143Z","iopub.status.idle":"2026-02-17T09:02:21.823705Z","shell.execute_reply.started":"2026-02-17T09:02:19.090110Z","shell.execute_reply":"2026-02-17T09:02:21.823039Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8dccb6f2ec0435b82f1b1f66516b5b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.jsonl:   0%|          | 0.00/33.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbb07de36ff42c68d671f5043a030bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.jsonl: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"623b25540045444e8b8b2f9c36e07f0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e46aabecf1ed47a3a4fe143fe9877f3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c2948e519d14411afd7d506492c7b58"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Tokenizer","metadata":{"id":"dd832b22"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n\n# Ensure we have a padding token (Qwen is a causal LM)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"f135bcd2","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:21.825508Z","iopub.execute_input":"2026-02-17T09:02:21.825755Z","iopub.status.idle":"2026-02-17T09:02:23.203936Z","shell.execute_reply.started":"2026-02-17T09:02:21.825731Z","shell.execute_reply":"2026-02-17T09:02:23.203342Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d70343c656f40f7adfc05604ea52328"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92d13d6431834f5ba32ab1ce9510a8f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2304e7ccfc44aeb96ab1f1d694320f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9a28368c199448c9f2fa7c7354c174e"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Model","metadata":{"id":"xrhA-fo_kx63"}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"Qwen/Qwen3-0.6B\",\n    num_labels=num_labels,\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False, # set to False for training\n    r=8, # dimension of the smaller matrices\n    lora_alpha=32, # scaling factor\n    lora_dropout=0.1 # dropout of LoRA layers\n)\n\nmodel = get_peft_model(model, lora_config)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)","metadata":{"id":"085db5df","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:23.204765Z","iopub.execute_input":"2026-02-17T09:02:23.205044Z","iopub.status.idle":"2026-02-17T09:02:27.468338Z","shell.execute_reply.started":"2026-02-17T09:02:23.205009Z","shell.execute_reply":"2026-02-17T09:02:27.467623Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f980f48f27f54bc29edf0991d82ca24b"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aba33c50cbf42aa8a78d66d1e412f1a"}},"metadata":{}},{"name":"stderr","text":"Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): Qwen3ForSequenceClassification(\n      (model): Qwen3Model(\n        (embed_tokens): Embedding(151936, 1024, padding_idx=151643)\n        (layers): ModuleList(\n          (0-27): 28 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n        )\n        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (rotary_emb): Qwen3RotaryEmbedding()\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=1024, out_features=4, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=1024, out_features=4, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Preprocess","metadata":{"id":"10387705"}},{"cell_type":"code","source":"max_length = 256\n\ndef preprocess(dataset_split, select_columns: List[str]):\n    def run_tokenizer(row):\n      return tokenizer(\n          row[\"text\"],\n          padding=\"max_length\",\n          truncation=True,\n          max_length=max_length,\n      )\n\n    def prepare_columns(row):\n      row[\"text\"] = row[\"title\"] + \" \" + row[\"description\"]\n      row[\"label\"] = row[\"label\"] - 1\n\n      return row\n\n    dataset_split = dataset_split.map(prepare_columns)\n\n    encoded_dataset = dataset_split.map(run_tokenizer, batched=True)\n\n    encoded_dataset = encoded_dataset.remove_columns(\n        [col for col in encoded_dataset.column_names if col not in select_columns]\n    )\n\n    return encoded_dataset.with_format(\"torch\")\n\ntrain_dataset = preprocess(dataset['train'], [\"input_ids\", \"attention_mask\", \"label\"])\nvalid_dataset = preprocess(dataset['valid'], [\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset = preprocess(dataset['test'], [\"label\", \"text\"])","metadata":{"id":"fe2221af","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:27.469395Z","iopub.execute_input":"2026-02-17T09:02:27.469750Z","iopub.status.idle":"2026-02-17T09:02:31.131107Z","shell.execute_reply.started":"2026-02-17T09:02:27.469717Z","shell.execute_reply":"2026-02-17T09:02:31.130361Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee8b366d124643ff86d0362158bc3e9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f299cf1ceddb48f19e27fd8247511a9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b669b418444152b5021af229559813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18756a8ca03f461083d7c2028ff7870a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce3de513274c454f8d2bb8e86d0e2e1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768006fb93124736be7d15f2e17928ea"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Metrics","metadata":{"id":"61ae7f96"}},{"cell_type":"code","source":"accuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    results = accuracy.compute(predictions=preds, references=labels)\n    results.update(\n        f1.compute(predictions=preds, references=labels, average=\"macro\")\n    )\n\n    return results","metadata":{"id":"6925906a","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:31.132072Z","iopub.execute_input":"2026-02-17T09:02:31.132302Z","iopub.status.idle":"2026-02-17T09:02:32.464413Z","shell.execute_reply.started":"2026-02-17T09:02:31.132280Z","shell.execute_reply":"2026-02-17T09:02:32.463900Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"198b32b95e0b46b1a8dccac7230332a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706d359b925a491693f2d10eab91c9c1"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{"id":"d8b54633"}},{"cell_type":"markdown","source":"## Training Arguments","metadata":{"id":"99a6b1ec"}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./qwen3_agnews_grid\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=0,\n    lr_scheduler_type=\"cosine\",\n    gradient_accumulation_steps=4,\n    fp16=False,\n    report_to=\"none\",\n)\n\ntrainer_kwargs = dict(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"id":"527fc936","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:32.465389Z","iopub.execute_input":"2026-02-17T09:02:32.465726Z","iopub.status.idle":"2026-02-17T09:02:32.509799Z","shell.execute_reply.started":"2026-02-17T09:02:32.465700Z","shell.execute_reply":"2026-02-17T09:02:32.509050Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Search Space","metadata":{"id":"329362be"}},{"cell_type":"code","source":"hyperparameters = {}\n\nidx = 0\nfor learning_rate in [1e-4, 1e-3]:\n    for num_train_epochs in [1, 2, 3]:\n        hyperparameters[idx] = {\n            \"learning_rate\": learning_rate,\n            \"num_train_epochs\": num_train_epochs\n        }\n\n        idx += 1\n","metadata":{"id":"08fd5ee0","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:32.510755Z","iopub.execute_input":"2026-02-17T09:02:32.511039Z","iopub.status.idle":"2026-02-17T09:02:32.515088Z","shell.execute_reply.started":"2026-02-17T09:02:32.511001Z","shell.execute_reply":"2026-02-17T09:02:32.514533Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Helper Functions","metadata":{"id":"208870ac"}},{"cell_type":"code","source":"def run_single_experiment(\n    base_training_args: TrainingArguments,\n    trainer_cls,\n    trainer_kwargs: Dict[str, Any],\n    hp_config: Dict[str, float|int],\n    idx: int\n) -> Dict[str, Any]:\n    \"\"\"\n    base_training_args: a TrainingArguments object with default values.\n    trainer_cls: usually `Trainer`.\n    trainer_kwargs: dict with keys like model, train_dataset, eval_dataset, tokenizer, compute_metrics.\n    hp_config: specific hyperparameters for this run (e.g. lr, epochs, batch size).\n    \"\"\"\n    # 1) Clone TrainingArguments and override selected fields\n    args_dict = base_training_args.to_dict()\n    for k, v in hp_config.items():\n        args_dict[k] = v\n\n    training_args = TrainingArguments(**args_dict)\n\n    # 2) Create a fresh Trainer (important if model should be reinitialized externally)\n    trainer = trainer_cls(\n        args=training_args,\n        **trainer_kwargs,\n    )\n\n    # 3) Train and evaluate\n    train_output = trainer.train()\n    eval_metrics = trainer.evaluate()\n\n    trainer.save_model(f\"checkpoint_{idx}\")\n\n    result = {\n        \"hp_config\": hp_config,\n        \"train_samples\": train_output.metrics.get(\"train_samples\", None),\n        \"eval_metrics\": eval_metrics,\n    }\n\n    return result\n\ndef grid_search_hyperparams(\n    base_training_args: TrainingArguments,\n    trainer_cls,\n    trainer_kwargs: Dict[str, Any],\n    hyperparameters: Dict[int, Dict[str, int|float]],\n    results_path: str = \"grid_search_results.jsonl\",\n) -> List[Dict[str, Any]]:\n    all_results = {}\n\n    os.makedirs(os.path.dirname(results_path) or \".\", exist_ok=True)\n\n    with open(results_path, \"w\", encoding=\"utf-8\") as f_out:\n        for idx, combo in hyperparameters.items():\n            print(\"\\n=== Running config:\", combo, \"===\")\n\n            result = run_single_experiment(\n                base_training_args=base_training_args,\n                trainer_cls=trainer_cls,\n                trainer_kwargs=deepcopy(trainer_kwargs),\n                hp_config=combo,\n                idx=idx\n            )\n\n            # Persist each result as one JSON line\n            f_out.write(json.dumps(result) + \"\\n\")\n            f_out.flush()\n\n            all_results[idx] = result\n\n    return all_results","metadata":{"id":"062e0479","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:32.515906Z","iopub.execute_input":"2026-02-17T09:02:32.516164Z","iopub.status.idle":"2026-02-17T09:02:32.530110Z","shell.execute_reply.started":"2026-02-17T09:02:32.516143Z","shell.execute_reply":"2026-02-17T09:02:32.529402Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Train","metadata":{"id":"bd6829fa"}},{"cell_type":"code","source":"results = grid_search_hyperparams(\n    base_training_args=training_arguments,\n    trainer_cls=Trainer,\n    trainer_kwargs=trainer_kwargs,\n    hyperparameters=hyperparameters,\n    results_path=\"grid_search_results.jsonl\",\n)","metadata":{"id":"fa47f6ab","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:02:32.531080Z","iopub.execute_input":"2026-02-17T09:02:32.531430Z","iopub.status.idle":"2026-02-17T11:32:51.682431Z","shell.execute_reply.started":"2026-02-17T09:02:32.531395Z","shell.execute_reply":"2026-02-17T11:32:51.681738Z"}},"outputs":[{"name":"stdout","text":"\n=== Running config: {'learning_rate': 0.0001, 'num_train_epochs': 1} ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 11:35, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.620500</td>\n      <td>0.696000</td>\n      <td>0.690634</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 01:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Running config: {'learning_rate': 0.0001, 'num_train_epochs': 2} ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [80/80 23:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.034547</td>\n      <td>0.798000</td>\n      <td>0.796459</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.894906</td>\n      <td>0.833000</td>\n      <td>0.830571</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 01:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Running config: {'learning_rate': 0.0001, 'num_train_epochs': 3} ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 35:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.995531</td>\n      <td>0.810000</td>\n      <td>0.807548</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.755172</td>\n      <td>0.857000</td>\n      <td>0.854984</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>4.666500</td>\n      <td>0.744250</td>\n      <td>0.862000</td>\n      <td>0.860443</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 01:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Running config: {'learning_rate': 0.001, 'num_train_epochs': 1} ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 11:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.667531</td>\n      <td>0.886000</td>\n      <td>0.885296</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 01:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Running config: {'learning_rate': 0.001, 'num_train_epochs': 2} ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [80/80 23:38, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.637293</td>\n      <td>0.888000</td>\n      <td>0.887133</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.610191</td>\n      <td>0.894000</td>\n      <td>0.892947</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 01:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Running config: {'learning_rate': 0.001, 'num_train_epochs': 3} ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 35:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.680750</td>\n      <td>0.895000</td>\n      <td>0.894203</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.635957</td>\n      <td>0.895000</td>\n      <td>0.894247</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.450900</td>\n      <td>0.645859</td>\n      <td>0.898000</td>\n      <td>0.896688</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 01:06]\n    </div>\n    "},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"best_config = max(results.items(), key=lambda r: r[1][\"eval_metrics\"].get(\"eval_f1\", 0.0))\n\nprint(\"Best index:\", best_config[0])\nprint(\"Best config:\", best_config[1][\"hp_config\"])\nprint(\"Best metrics:\", best_config[1][\"eval_metrics\"])","metadata":{"trusted":true,"id":"6fpkVLBxwETe","execution":{"iopub.status.busy":"2026-02-17T11:32:51.683413Z","iopub.execute_input":"2026-02-17T11:32:51.683762Z","iopub.status.idle":"2026-02-17T11:32:51.689146Z","shell.execute_reply.started":"2026-02-17T11:32:51.683735Z","shell.execute_reply":"2026-02-17T11:32:51.688539Z"}},"outputs":[{"name":"stdout","text":"Best index: 5\nBest config: {'learning_rate': 0.001, 'num_train_epochs': 3}\nBest metrics: {'eval_loss': 0.6458593606948853, 'eval_accuracy': 0.898, 'eval_f1': 0.8966882356897352, 'eval_runtime': 67.11, 'eval_samples_per_second': 14.901, 'eval_steps_per_second': 0.939, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"466e425e"}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    f\"./checkpoint_{best_config[0]}\",\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n    pad_token_id=tokenizer.pad_token_id,\n    num_labels=num_labels,\n    \n)\n\nresults = evaluator(\"text-classification\").compute(\n    model_or_pipeline=pipeline(\"text-classification\", model=model, tokenizer=tokenizer),\n    data=test_dataset,\n    metric=evaluate.load(\"f1\"),\n    input_column=\"text\",\n    label_column=\"label\",\n    strategy=\"bootstrap\",\n    n_resamples=200\n)\n\nprint(results)","metadata":{"id":"11e427eb","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T11:43:54.586755Z","iopub.execute_input":"2026-02-17T11:43:54.587628Z","iopub.status.idle":"2026-02-17T11:46:04.928574Z","shell.execute_reply.started":"2026-02-17T11:43:54.587595Z","shell.execute_reply":"2026-02-17T11:46:04.927580Z"}},"outputs":[{"name":"stderr","text":"Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nDevice set to use cuda:0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3458779000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m results = evaluator(\"text-classification\").compute(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel_or_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/evaluate/evaluator/text_classification.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, second_input_column, label_column, label_mapping)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Compute metrics from references and predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         metric_results = self.compute_metric(\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mmetric_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/evaluate/evaluator/base.py\u001b[0m in \u001b[0;36mcompute_metric\u001b[0;34m(self, metric, metric_inputs, strategy, confidence_level, n_resamples, random_state)\u001b[0m\n\u001b[1;32m    525\u001b[0m     ):\n\u001b[1;32m    526\u001b[0m         \u001b[0;34m\"\"\"Compute and return metrics.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmetric_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRIC_KWARGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bootstrap\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_nested_string_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowInvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   2091\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2092\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2093\u001b[0;31m             \u001b[0mencoded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;31m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encode_example\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m     \u001b[0;31m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_example\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'LABEL_1'"],"ename":"ValueError","evalue":"invalid literal for int() with base 10: 'LABEL_1'","output_type":"error"}],"execution_count":16}]}